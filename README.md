Next Word Prediction using LSTM & GRU

This project demonstrates a deep learning approach to next-word prediction using LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks, key architectures in natural language processing (NLP). The model is trained on a text corpus to learn patterns in word sequences and predict the most probable next word given an input sequence.

The workflow includes text preprocessing (tokenization, sequence padding), model training with LSTM and GRU layers, and evaluation using sample inputs to demonstrate prediction accuracy. This project highlights the differences between LSTM and GRU in handling long-term dependencies in text.

It serves as a practical example for understanding sequence modeling, recurrent neural networks, and the applications of NLP in autocomplete systems, chatbots, and text generation tools. The repository includes all scripts, trained models, and instructions to run predictions on custom text.
